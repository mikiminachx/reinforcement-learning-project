{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8e05f-c735-488a-b459-fa397b494b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "# Define a function for parallelized fine-tuning\n",
    "def fine_tune_model(lr, epsilon, discount_factor, num_episodes, loaded_model, env):\n",
    "    # Clone the loaded model to start with a fresh copy for fine-tuning\n",
    "    fine_tuned_model = tf.keras.models.clone_model(loaded_model)\n",
    "    fine_tuned_model.set_weights(loaded_model.get_weights())  # Copy weights\n",
    "\n",
    "    # Set hyperparameters for fine-tuning\n",
    "    fine_tuned_model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "    epsilon_fine_tune = epsilon\n",
    "    discount_factor_fine_tune = discount_factor\n",
    "\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Fine-tune the loaded model\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = fine_tuned_model.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Fine-tune the model by updating its weights\n",
    "            # Update the model's weights based on your fine-tuning logic here\n",
    "            # For example, you can use model.train(state, action, reward, next_state, done)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "        # Update the target model\n",
    "        if episode % 100 == 0:\n",
    "            fine_tuned_model.update_target_model()\n",
    "\n",
    "    # Calculate the average reward for the fine-tuned model\n",
    "    average_reward = np.mean(episode_rewards)\n",
    "\n",
    "    return {\n",
    "        'lr': lr,\n",
    "        'epsilon': epsilon_fine_tune,\n",
    "        'discount_factor': discount_factor_fine_tune,\n",
    "        'average_reward': average_reward\n",
    "    }\n",
    "\n",
    "# Use multiprocessing to fine-tune models in parallel\n",
    "num_processes = multiprocessing.cpu_count()  # Use all available CPU cores\n",
    "\n",
    "# Create a list of hyperparameters to search\n",
    "hyperparameters_to_search = [(lr, epsilon, discount_factor, num_episodes, loaded_model, env)\n",
    "                             for lr in learning_rates\n",
    "                             for epsilon in epsilons\n",
    "                             for discount_factor in discount_factors]\n",
    "\n",
    "# Parallelize the fine-tuning process\n",
    "with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "    results = pool.starmap(fine_tune_model, hyperparameters_to_search)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "best_result = max(results, key=lambda x: x['average_reward'])\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_result)\n",
    "print(\"Best Average Reward:\", best_result['average_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc1363-f478-497b-80a1-0244a239d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best-performing model\n",
    "best_fine_tuned_model = fine_tuned_model  # Replace with the actual best-performing fine-tuned model\n",
    "best_fine_tuned_model.save('best_fine_tuned_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
